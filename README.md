
# CSE425: Multi-Modal Music Representation Learning using VAEs

This repository contains the implementation for a course project on **unsupervised music representation learning and clustering** using **Variational Autoencoders (VAEs)**.  
The project explores progressively complex settings, starting from a basic VAE to a multi-modal VAE and finally a Beta-VAE for disentangled latent representations.



---

## ğŸ“‚ Project Structure

```

CSE425/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ extract_features.py        # Audio + lyrics feature extraction
â”‚   â”œâ”€â”€ train_vae.py               # Basic VAE (Easy Task)
â”‚   â”œâ”€â”€ clustering_easy.py         # PCA & VAE clustering (Easy Task)
â”‚   â”œâ”€â”€ train_multimodal_vae.py    # Multi-modal VAE (Medium Task)
â”‚   â”œâ”€â”€ clustering.py              # Multi-modal clustering (Medium Task)
â”‚   â”œâ”€â”€ train_beta_vae.py          # Beta-VAE (Hard Task)
â”‚   â”œâ”€â”€ clustering_hard.py         # Advanced clustering + metrics
â”‚   â””â”€â”€ visualize.py               # UMAP / t-SNE visualizations
â”‚
â”œâ”€â”€ features/
â”‚   â”œâ”€â”€ audio/                     # Extracted audio features
â”‚   â””â”€â”€ lyrics/                    # Extracted lyric embeddings
â”‚
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ easy/                      # Easy task results
â”‚   â”œâ”€â”€ medium/                    # Medium task results
â”‚   â””â”€â”€ hard/                      # Hard task results
â”‚
â”œâ”€â”€ figures/                       # Figures used in the report
â”‚
â”œâ”€â”€ Dataset/                       # (Not included in repo)
â”‚   â”œâ”€â”€ Audio/
â”‚   â””â”€â”€ CSV/
â”‚
â””â”€â”€ requirements.txt
â””â”€â”€ README.md

````

---

## ğŸ“¦ Dataset

The dataset used in this project is **not publicly included** in the repository.

It consists of:
- Multilingual songs
- Fields: `track_name`, `lyrics`, `genre`
- Audio generated from lyrics using a text-to-speech pipeline

ğŸ“© **If you require access to the dataset for academic purposes, please contact me via GitHub or Email.**

---

## âš™ï¸ Environment Setup

### 1ï¸âƒ£ Clone the repository
```bash
git clone https://github.com/Imraj-Rabbani/CSE425.git
cd CSE425
````

### 2ï¸âƒ£ Create a virtual environment (recommended)

```bash
python3 -m venv venv
source venv/bin/activate
```

### 3ï¸âƒ£ Install dependencies

```bash
pip install -r requirements.txt
```


---

## ğŸ”§ Step 1: Feature Extraction (Required for All Tasks)

Before running any task, extract features from audio and lyrics:

```bash
python3 src/extract_features.py
```

This will generate:

* `features/audio/*.npy`
* `features/lyrics/*.npy`

---

## ğŸŸ¢ Easy Task: Basic VAE + Clustering

### Objective

* Train a basic VAE on audio features
* Perform clustering using K-Means
* Compare with PCA baseline
* Visualize latent space

### Commands

```bash
python3 src/train_vae.py
python3 src/clustering_easy.py
```

### Outputs

* VAE model checkpoint
* PCA vs VAE clustering metrics
* Latent space visualizations (UMAP / t-SNE)

---

## ğŸŸ¡ Medium Task: Multi-Modal VAE (Audio + Lyrics)

### Objective

* Learn joint representations using audio and lyrics
* Perform clustering with multiple algorithms
* Analyze multi-modal latent space

### Commands

```bash
python3 src/train_multimodal_vae.py
python3 src/clustering.py
python3 src/visualize.py
```

### Outputs

* Multi-modal VAE model
* Clustering metrics (Silhouette, Davies-Bouldin)
* Multi-modal latent space plots

---

## ğŸ”´ Hard Task: Beta-VAE + Advanced Evaluation

### Objective

* Train a Beta-VAE for disentangled representations
* Perform multi-modal clustering
* Evaluate using advanced metrics (ARI, NMI, Purity)
* Analyze reconstructions and genre alignment

### Commands

```bash
python3 src/train_beta_vae.py
python3 src/clustering_hard.py
python3 src/visualize.py
```

### Outputs

* Beta-VAE model
* Advanced clustering metrics
* Disentangled latent space visualizations
* Reconstruction examples

---

## ğŸ“Š Evaluation Metrics Used

* Silhouette Score
* Calinskiâ€“Harabasz Index
* Daviesâ€“Bouldin Index
* Normalized Mutual Information (NMI)
* Adjusted Rand Index (ARI)
* Cluster Purity

---

## ğŸ“ˆ Visualizations

All plots generated during experiments are saved in:

```
figures/
```

These include:

* Latent space UMAP/t-SNE plots
* Genre distribution across clusters
* Reconstruction comparisons

---

## ğŸ” Reproducibility

* All experiments are deterministic given the same dataset
* Scripts are modular and task-specific
* Clear separation between Easy, Medium, and Hard tasks

---

## ğŸ“Œ Notes

* Training was performed on CPU-based systems
* Results may vary slightly depending on hardware and dataset size
* Genre labels are used **only for evaluation**, not during training

---

## ğŸ“« Contact

For dataset access or questions:

* GitHub: [https://github.com/Imraj-Rabbani](https://github.com/Imraj-Rabbani)
* Email: imraj.rabbani@g.bracu.ac.bd
